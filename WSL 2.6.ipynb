{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c40bbe8",
   "metadata": {},
   "source": [
    "# WSL Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf44fb",
   "metadata": {},
   "source": [
    "## Build athlete dataset with web scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db422db",
   "metadata": {},
   "source": [
    "### Create a list of links to individual athlete pages using Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6497504",
   "metadata": {},
   "source": [
    "#### Using Selenium allows for a different list to be generated by each year without that data having to be provided. For now, we are looking at the last 10 years of the tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ['2023', '2022', '2021', '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013']\n",
    "for years in year:\n",
    "    url = \"https://www.worldsurfleague.com/athletes/tour/mct?year=\"+years\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e997be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "year = ['2023', '2022', '2021', '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013']\n",
    "for years in year:\n",
    "    url = \"https://www.worldsurfleague.com/athletes/tour/mct?year=\"+years\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"detach\", True)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
    "                          options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # identify element with link to athlete\n",
    "    lnks = driver.find_elements(\"class name\", \"athlete-name\")\n",
    "\n",
    "    for lnk in lnks:\n",
    "        # get_attribute() to get all href\n",
    "        links.append(lnk.get_attribute(\"href\"))\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(set(links))\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e221d43",
   "metadata": {},
   "source": [
    "### Scrape athlete data using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b029d26",
   "metadata": {},
   "source": [
    "#### These are a few metrics to start. Specific data for each stop on tour will be examined after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83170375",
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_information = []\n",
    "\n",
    "for link in links:\n",
    "    url = link\n",
    "    \n",
    "    #send an http request to the URL\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    #metrics\n",
    "\n",
    "    name = soup.find(\"div\", {\"class\": \"avatar-text-primary\"}).get_text(strip=True)\n",
    "    \n",
    "    nationality = soup.find(\"div\", class_= \"country-name\").get_text(strip=True)\n",
    "    \n",
    "    try:\n",
    "        stance = soup.find(\"div\", class_=\"label\", text=\"Stance\").find_next(\"div\", class_=\"value\").get_text(strip=True)\n",
    "    except:\n",
    "        stance = \"Stance not found.\"\n",
    "        \n",
    "    last_ranking = soup.find(\"div\", class_=\"value\").get_text(strip=True)\n",
    "    \n",
    "    try:\n",
    "        age = soup.find(\"div\", class_=\"label\", text=\"Age\").find_next(\"span\", class_=\"imperial\").get_text(strip=True)\n",
    "    except:\n",
    "        age = \"Age not found.\"\n",
    "        \n",
    "    first_season = soup.find(\"div\", class_=\"label\", text=\"First season\").find_next(\"div\", class_=\"value\").get_text(strip=True)\n",
    "    \n",
    "    athlete_information.append({'Name': name, 'Nationality': nationality, 'Stance': stance, 'Last_Ranking': last_ranking, 'Age': age, 'First Season': first_season})\n",
    "    \n",
    "driver.quit()\n",
    "    \n",
    "#print(athlete_information)   \n",
    "\n",
    "athlete_df = pd.DataFrame(athlete_information)\n",
    "\n",
    "athlete_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_df.to_excel(r\"/Users/carmenhoyt/Downloads/athlete_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8fd00",
   "metadata": {},
   "source": [
    "#### Event data for each athlete. To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9634baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_links = []\n",
    "\n",
    "for link in links:\n",
    "    #url = link\n",
    "    url = \"https://www.worldsurfleague.com/athletes/8801/morgan-cibilic\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"detach\", True)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
    "                          options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "       \n",
    "    year_option = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "      \n",
    "    for x in year_option:\n",
    "        #try:\n",
    "        \n",
    "        select = Select(driver.findElement(By.xpath(\"//*[@id='primary']/div/div/div[2]/div/div[1]/div[3]/div[1]/div[2]/form[2]/select\")))\n",
    "        select.select_by_index(2)\n",
    "        \n",
    "        #driver.find_elements(\"//*[@id='primary']/div/div/div[2]/div/div[1]/div[3]/div[1]/div[2]/form[2]/select\").click()\n",
    "        #xpath = \"//*[@id='primary']/div/div/div[2]/div/div[1]/div[3]/div[1]/div[2]/form[2]/select/option[2]\"\n",
    "        #xpath = \"//*[@id='primary']/div/div/div[2]/div/div[1]/div[3]/div[1]/div[2]/form[2]/select/option[\"+events+\"]\"    \n",
    "        #driver.find_elements(\"xpath\", xpath).click()\n",
    "        time.sleep(5)\n",
    "        current_url = driver.current_url\n",
    "        tables = pd.read_html(current_url.text)\n",
    "        name = soup.find(\"div\", {\"class\": \"avatar-text-primary\"}).get_text(strip=True)\n",
    "        year = soup.find(\"div\", {\"class\": \"on-change-filter\"}).get_text(strip=True)\n",
    "    \n",
    "        #assuming the first table on the page contains the desired data\n",
    "        if tables:\n",
    "            #Get the first table as a DataFrame\n",
    "            df = tables[0]\n",
    "            df['Surfer'] = name\n",
    "            df['Year'] = year\n",
    "        \n",
    "            print(df)\n",
    "        \n",
    "        else:\n",
    "            print(\"No tables found on the page.\")\n",
    "            #ath_events= driver.current_url\n",
    "            #event_links.append(ath_events)\n",
    "            \n",
    "        #except:\n",
    "            #print(\"Could not find.\")\n",
    "            #continue\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    print(event_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673daea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.worldsurfleague.com/athletes/4133/ethan-ewing'\n",
    "\n",
    "for link in links:\n",
    "    \n",
    "    url = link\n",
    "    \n",
    "    #send an http request to the URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")   \n",
    "    \n",
    "    #extract data from the table using Pandas read_html() function\n",
    "    tables = pd.read_html(response.text)\n",
    "    name = soup.find(\"div\", {\"class\": \"avatar-text-primary\"}).get_text(strip=True)\n",
    "    year = soup.find()\n",
    "    \n",
    "    #assuming the first table on the page contains the desired data\n",
    "    if tables:\n",
    "        #Get the first table as a DataFrame\n",
    "        df = tables[0]\n",
    "        df['Surfer'] = name\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ac9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
